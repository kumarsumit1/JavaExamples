# Redshift
It is essentially a MPP based Postgres SQL.

Performance Tips :
https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/

## CREATE TABLE
**Topics**
- Syntax
- Parameters
- Usage Notes
  
Creates a new table in the current database. The owner of this table is the user of the CREATE TABLE command.

### Syntax

```
CREATE [ [LOCAL ] { TEMPORARY | TEMP } ] TABLE 
[ IF NOT EXISTS ] table_name
( { column_name data_type [column_attributes] [ column_constraints ] 
  | table_constraints
  | LIKE parent_table [ { INCLUDING | EXCLUDING } DEFAULTS ] } 
  [, ... ]  )
[ BACKUP { YES | NO } ]
[table_attribute]

where column_attributes are:
  [ DEFAULT default_expr ]
  [ IDENTITY ( seed, step ) ] 
  [ GENERATED BY DEFAULT AS IDENTITY ( seed, step ) ]             
  [ ENCODE encoding ] 
  [ DISTKEY ]
  [ SORTKEY ]

and column_constraints are:
  [ { NOT NULL | NULL } ]
  [ { UNIQUE  |  PRIMARY KEY } ]
  [ REFERENCES reftable [ ( refcolumn ) ] ] 

and table_constraints  are:
  [ UNIQUE ( column_name [, ... ] ) ]
  [ PRIMARY KEY ( column_name [, ... ] )  ]
  [ FOREIGN KEY (column_name [, ... ] ) REFERENCES reftable [ ( refcolumn ) ] 


and table_attributes are:
  [ DISTSTYLE { AUTO | EVEN | KEY | ALL } ] 
  [ DISTKEY ( column_name ) ]
  [ [COMPOUND | INTERLEAVED ] SORTKEY ( column_name [, ...] ) ]
```

### Parameters

LOCAL   
Optional. Although this keyword is accepted in the statement, it has no effect in Amazon Redshift.

TEMPORARY | TEMP   
- Keyword that creates a temporary table that is visible only within the current session. 
- The temporary table can have the same name as a permanent table. The temporary table is created in a separate, session-specific schema. (You can't specify a name for this schema.)  

IF NOT EXISTS  
Clause that indicates that if the specified table already exists, the command should make no changes and return a message that the table exists, rather than terminating with an error.

**table_name**   
Name of the table to be created.  
If you specify a table name that begins with '# ', the table is created as a temporary table. The following is an example:  

```
create table #newtable (id int);
```

**column_name**   
Name of a column to be created in the new table. The maximum length for the column name is 127 bytes; longer names are truncated to 127 bytes. 
 
**data_type**   
The data type of the column being created.  
The following Data Types are supported:  
+ SMALLINT \(INT2\)
+ INTEGER \(INT, INT4\)
+ BIGINT \(INT8\)
+ DECIMAL \(NUMERIC\)
+ REAL \(FLOAT4\)
+ DOUBLE PRECISION \(FLOAT8\)
+ BOOLEAN \(BOOL\)
+ CHAR \(CHARACTER\)
+ VARCHAR \(CHARACTER VARYING\)
+ DATE
+ TIMESTAMP
+ TIMESTAMPTZ
+ GEOMETRY


DEFAULT *default_expr*   
- Clause that assigns a default data value for the column. 
- If no default value is specified, the default value for the column is null.  


IDENTITY(*seed*, *step*)   
- An IDENTITY column contains unique autogenerated values. 
- The data type for an IDENTITY column must be either INT or BIGINT.   
- When you add rows using an `INSERT` or `INSERT INTO [tablename] VALUES()` statement, these values start with the value specified as *seed* and increment by the number specified as *step*.   
- When you load the table using an `INSERT INTO [tablename] SELECT * FROM` or `COPY` statement, the data is loaded in parallel and distributed to the node slices. To be sure that the identity values are unique, Amazon Redshift skips a number of values when creating the identity values. Identity values are unique, but the order might not match the order in the source files. 


Keep in mind the following about default identity columns:   
+ Default identity columns are NOT NULL. NULL can't be inserted\.
+ To insert a generated value into a default identity column, use the keyword `DEFAULT`. 

  ```
  INSERT INTO tablename (identity-column-name) VALUES (DEFAULT);
  ```
+ Overriding values of a default identity column doesn't affect the next generated value. 
+ You can't add a default identity column with the ALTER TABLE ADD COLUMN statement. 
+ You can append a default identity column with the ALTER TABLE APPEND statement. 

ENCODE *encoding*   
Compression encoding for a column. If no compression is selected, Amazon Redshift automatically assigns compression encoding as follows:  
+ All columns in temporary tables are assigned RAW compression by default.
+ Columns that are defined as sort keys are assigned RAW compression.
+ Columns that are defined as BOOLEAN, REAL, DOUBLE PRECISION, or GEOMETRY data type are assigned RAW compression.
+ Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, CHAR, VARCHAR, DATE, TIMESTAMP, or TIMESTAMPTZ are assigned LZO compression.

NOTE: If you don't want a column to be compressed, explicitly specify RAW encoding.

The following compression encodings are supported:  
+ AZ64
+ BYTEDICT
+ DELTA
+ DELTA32K
+ LZO
+ MOSTLY8
+ MOSTLY16
+ MOSTLY32
+ RAW (no compression)
+ RUNLENGTH
+ TEXT255
+ TEXT32K
+ ZSTD

DISTKEY  
- Keyword that specifies that the column is the distribution key for the table. 
- Only one column in a table can be the distribution key. 
- You can use the DISTKEY keyword after a column name or as part of the table definition by using the DISTKEY (*column_name*) syntax. 

DISTKEY ( *column_name* )  
Constraint that specifies the column to be used as the distribution key for the table. You can use the DISTKEY keyword after a column name or as part of the table definition, by using the DISTKEY (*column_name*) syntax. Either method has the same effect. 


DISTSTYLE { AUTO | EVEN | KEY | ALL }  
Keyword that defines the data distribution style for the whole table. Amazon Redshift distributes the rows of a table to the compute nodes according to the distribution style specified for the table. The default is AUTO.  

The distribution style that you select for tables affects the overall performance of your database.  Possible distribution styles are as follows:  
+ AUTO: Amazon Redshift assigns an optimal distribution style based on the table data. For example, if AUTO distribution style is specified, Amazon Redshift initially assigns ALL distribution to a small table, then changes the table to EVEN distribution when the table grows larger. The change in distribution occurs in the background, in a few seconds. Amazon Redshift never changes the distribution style from EVEN to ALL. To view the distribution style applied to a table, query the PG_CLASS system catalog table.  
+ EVEN: The data in the table is spread evenly across the nodes in a cluster in a round-robin distribution. Row IDs are used to determine the distribution, and roughly the same number of rows are distributed to each node. 
+ KEY: The data is distributed by the values in the DISTKEY column. When you set the joining columns of joining tables as distribution keys, the joining rows from both tables are collocated on the compute nodes. When data is collocated, the optimizer can perform joins more efficiently. ***If you specify DISTSTYLE KEY, you must name a DISTKEY column, either for the table or as part of the column definition.*** 
+  ALL: A copy of the entire table is distributed to every node. This distribution style ensures that all the rows required for any join are available on every node, but it multiplies storage requirements and increases the load and maintenance times for the table. ALL distribution can improve execution time when used with certain dimension tables where KEY distribution isn't appropriate, but performance improvements must be weighed against maintenance costs.

SORTKEY  
Keyword that specifies that the column is the sort key for the table. When data is loaded into the table, the data is sorted by one or more columns that are designated as sort keys. 
- You can use the SORTKEY keyword after a column name to specify a single-column sort key, or you can specify one or more columns as sort key columns for the table by using the SORTKEY (*column_name* [, ...]) syntax. Only compound sort keys are created with this syntax. 
- If you don't specify any sort keys, the table isn't sorted.
- You can define a maximum of 400 SORTKEY columns per table.

[ { COMPOUND | INTERLEAVED } ] SORTKEY (* column_name* [,... ] )  
Specifies one or more sort keys for the table. When data is loaded into the table, the data is sorted by the columns that are designated as sort keys. 
- You can use the SORTKEY keyword after a column name to specify a single-column sort key, or you can specify one or more columns as sort key columns for the table by using the `SORTKEY (column_name [ , ... ] )` syntax.   
- You can optionally specify COMPOUND or INTERLEAVED sort style. The default is COMPOUND.   
- If you don't specify any sort keys, the table isn't sorted by default. You can define a maximum of 400 COMPOUND SORTKEY columns or 8 INTERLEAVED SORTKEY columns per table.
- COMPOUND  
  - Specifies that the data is sorted using a compound key made up of all of the listed columns, in the order they are listed. 
  - A compound sort key is most useful when a query scans rows according to the order of the sort columns. 
  - The performance benefits of sorting with a compound key decrease when queries rely on secondary sort columns. 
  - You can define a maximum of 400 COMPOUND SORTKEY columns per table.   
- INTERLEAVED  
  - Specifies that the data is sorted using an interleaved sort key. 
  - A maximum of eight columns can be specified for an interleaved sort key.
  - An interleaved sort gives equal weight to each column, or subset of columns, in the sort key, so queries don't depend on the order of the columns in the sort key. 
  - When a query uses one or more secondary sort columns, interleaved sorting significantly improves query performance. 
  - Interleaved sorting carries a small overhead cost for data loading and vacuuming operations.
  - Don’t use an interleaved sort key on columns with monotonically increasing attributes, such as identity columns, dates, or timestamps.

UNIQUE  
Keyword that specifies that the column can contain only unique values. The behavior of the unique table constraint is the same as that for column constraints, with the additional capability to span multiple columns. To define a unique table constraint, use the UNIQUE ( *column_name* [, ... ] ) syntax.  
***Unique constraints are informational and aren't enforced by the system.***

UNIQUE ( *column_name* [, ...] )  
Constraint that specifies that a group of one or more columns of a table can contain only unique values. The behavior of the unique table constraint is the same as that for column constraints, with the additional capability to span multiple columns. ***In the context of unique constraints, null values aren't considered equal.*** Each unique table constraint must name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table.
***Unique constraints are informational and aren't enforced by the system.***

PRIMARY KEY  
Keyword that specifies that the column is the primary key for the table. Only one column can be defined as the primary key by using a column definition. To define a table constraint with a multiple-column primary key, use the PRIMARY KEY ( *column_name* [, ... ] ) syntax.  
***Primary key constraints are informational only. They aren't enforced by the system, but they are used by the planner.***

References *reftable* [ ( *refcolumn* ) ]  
Clause that specifies a foreign key constraint, which implies that the column must contain only values that match values in the referenced column of some row of the referenced table. The referenced columns should be the columns of a unique or primary key constraint in the referenced table.   
***Foreign key constraints are informational only. They aren't enforced by the system, but they are used by the planner.***

FOREIGN KEY ( *column_name* [, ... ] ) REFERENCES *reftable* [ ( *refcolumn* ) ]   
Constraint that specifies a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If *refcolumn* is omitted, the primary key of *reftable* is used. The referenced columns must be the columns of a unique or primary key constraint in the referenced table.  
***Foreign key constraints are informational only. They aren't enforced by the system, but they are used by the planner.***

LIKE *parent_table* [ { INCLUDING | EXCLUDING } DEFAULTS ]   
A clause that specifies an existing table from which the new table automatically copies column names, data types, and NOT NULL constraints. 
- The new table and the parent table are decoupled, and any changes made to the parent table aren't applied to the new table. 
- The default behavior is to exclude default expressions, so that all columns of the new table have null defaults.   
- Default expressions for the copied column definitions are copied only if INCLUDING DEFAULTS is specified. 
- Tables created with the LIKE option don't inherit primary and foreign key constraints. 
- Distribution style, sort keys,BACKUP, and NULL properties are inherited by LIKE tables, but you can't explicitly set them in the CREATE TABLE ... LIKE statement.

BACKUP { YES | NO }   
A clause that specifies whether the table should be included in automated and manual cluster snapshots. For tables, such as staging tables, that don't contain critical data, specify BACKUP NO to save processing time when creating snapshots and restoring from snapshots and to reduce storage space on Amazon Simple Storage Service. The BACKUP NO setting has no effect on automatic replication of data to other nodes within the cluster, so tables with BACKUP NO specified are restored in a node failure. ***The default is BACKUP YES.***


## Maintainance Commands

### Analyze
    The ANALYZE operation updates the statistical metadata that the query planner uses to choose optimal plans. 
    
    In most cases, you don't need to explicitly run the ANALYZE command. Amazon Redshift monitors changes to your workload and automatically updates statistics in the background. In addition, the COPY command performs an analysis automatically when it loads data into an empty table.

To explicitly analyze a table or the entire database, run the ANALYZE command. 

### Vacuum
    Re-sorts rows and reclaims space in either a specified table or all tables in the current database. 
    Amazon Redshift automatically sorts data and runs VACUUM DELETE in the background. This lessens the need to run the VACUUM command. 
    Users can access tables while they are being vacuumed. You can perform queries and write operations while a table is being vacuumed, but when data manipulation language (DML) commands and a vacuum run concurrently, both might take longer. If you execute UPDATE and DELETE statements during a vacuum, system performance might be reduced. VACUUM DELETE temporarily blocks update and delete operations. 

    
``` Syntax   
VACUUM [ FULL | SORT ONLY | DELETE ONLY | REINDEX ] 
[ [ table_name ] [ TO threshold PERCENT ] [ BOOST ] ]
```

## Misc Commands

### COPY
    Loads data into a table from data files or from an Amazon DynamoDB table. The files can be located in an Amazon Simple Storage Service (Amazon S3) bucket, an Amazon EMR cluster, or a remote host that is accessed using a Secure Shell (SSH) connection. The simplest COPY command uses the following format.
```
    COPY table-name 
    FROM data-source
    authorization;
```


    An anti-pattern is to insert data directly into Amazon Redshift, with single record inserts or the use of a multi-value INSERT statement. This allows you to insert up to 16 MB of data at one time. These are leader node–based operations and can create performance bottlenecks by maxing out the leader node network as the leader distributes the data to the compute nodes.

  - The COPY command appends the new input data to any existing rows in the table.
  - Amazon Redshift Spectrum external tables are read-only. You can't COPY to an external table. 
### UNLOAD
    Unloads the result of a query to one or more text or Apache Parquet files on Amazon S3, using Amazon S3 server-side encryption (SSE-S3). 

    Amazon Redshift can export SQL statement output to S3 in a massively parallel fashion. This technique greatly improves the export performance and lessens the impact of running the data through the leader node. You can compress the exported data on its way off the Amazon Redshift cluster. As the size of the output grows, so does the benefit of using this feature.

## Tools

### Amazon Redshift Advisor

### Amazon Redshift Spectrum

### workload management (WLM) Queues

----------------------------------------------------------------------------------------------------------
# [ Snowflake vs. Redshift ](https://www.stitchdata.com/resources/snowflake-vs-redshift/)

How do these two cloud data warehouse solutions compare? Here's a quick guide:

Snowflake:

    Pay separately for compute and storage
    More robust support for JSON-based functions
    Tier-based packages
    Security and compliance options vary by tier
    Unique architecture designed to scale on the web
    More automated database maintenance features

Redshift:

    Deep discounts on long-term commitments
    More unified offer package
    Security and compliance enforced in a comprehensive fashion for all users
    Machine learning engine
    More hands-on maintenance

Bottom line: Snowflake is a better platform to start and grow with. Redshift is a solid cost-efficient solution for enterprise-level implementations. This said — do your homework!


Redshift vs. Snowflake: Which warehouse makes sense for you?

Further comparison between these two data warehouse solutions illustrates how they’re suited for different needs:

    Features: bundled or not? Redshift bundles compute and storage to provide the immediate potential to scale to an enterprise-level data warehouse. But by splitting computation and storage and offering tiered editions, Snowflake provides businesses the flexibility to purchase only the features they need while preserving the potential to scale.
    
	JSON: dealbreaker or no big deal? When it comes to JSON storage, Snowflake’s support is decidedly more robust than Redshift. This means that with Snowflake you can store and query JSON with native, built-in functions. When JSON is loaded into Redshift, it’s split into strings, which makes it harder to work with and query.
    
	Security: everything you could ever need, or only what your business needs? Redshift includes a deep bench of customizable encryption solutions, but Snowflake provides security and compliance features oriented to its specific editions so that you have the level of protection most relevant to your data strategy.
    Data duties: automated or hands-on? Redshift requires more hands-on maintenance for a greater range of tasks that can’t be automated, such as data vacuuming and compression. Snowflake has the advantage in this regard: it automates more of these issues, saving significant time in diagnosing and resolving issues.






Data Warehousing with Amazon Redshift
https://www.youtube.com/watch?v=TFLoCLXulU0

Deep Dive on Amazon Redshift - AWS Online Tech Talks
https://www.youtube.com/watch?v=Hur-p3kGDTA

AWS re:Invent 2015 | (BDT401) Amazon Redshift Deep Dive: Tuning and Best Practices
https://www.youtube.com/watch?v=fmy3jCxUliM


https://github.com/awsdocs/amazon-redshift-developer-guide/blob/master/doc_source/c-who-should-use-this-guide.md


Amazon Redshift Best Practices for Designing Tables
https://github.com/awsdocs/amazon-redshift-developer-guide/blob/master/doc_source/c_designing-tables-best-practices.md


https://github.com/awslabs/amazon-redshift-utils

https://www.youtube.com/watch?v=xSQNIk4btCI&list=PLh5v4I2M5t-WrP-HM_yfn_M98oN8znz6C


Upto 9900 tables in a DB

##Create Table :

Types of Table
 Temporary table : will be available in only the session. One can also use hash(#) to create temporary table.
 
 Local table : Optional. Although this keyword is accepted in the statement, it has no effect in Amazon Redshift.
 
 
Data Types:

identity column :


Encode : 7 types of encoding 


Distkey : Distribution key, sotres data on same node depending on key


DistStyle :

Sortkye :(Compound,Interleaved)

Uniuqe Column : the command is not enforced by redshift and despite the value it will allow insertion.

Primary Key : Even PK is not enforced by Redshift.

Foregion Key : Not enforced by Redshift

Like : creates similar table.


Column Encoding :
https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html

Truncate : Release all the un occupied space at the same time

Delete : Release all the un occupied space after vaccume only