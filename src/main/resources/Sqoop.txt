# Sqoop

## Syntax structure
$ sqoop help import \
usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]

***Note that generic Hadoop arguments are preceeded by a single dash character (-), whereas tool-specific arguments start with two dashes (--), unless they are single character arguments such as -P.***

Generic options supported are \
-conf <configuration file>     specify an application configuration file \
-D <property=value>            use value for given property \
-fs <local|namenode:port>      specify a namenode \
-jt <local|jobtracker:port>    specify a job tracker \
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster \
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath. \
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines. 

Common arguments: \
--connect <jdbc-uri>     Specify JDBC connect string \
--connect-manager <class-name>     Specify connection manager class to use \
--driver <class-name>    Manually specify JDBC driver class to use \
--hadoop-mapred-home `<dir>`      Override $HADOOP_MAPRED_HOME \
--help                   Print usage instructions \
--password-file          Set path for file containing authentication password \
-P                       Read password from console \
--password <password>    Set authentication password \
--username <username>    Set authentication username \
--verbose                Print more information while working

The -conf, -D, -fs and -jt arguments control the configuration and Hadoop server settings. ***For example, the -D mapred.job.name=<job_name> can be used to set the name of the MR job that Sqoop launches***, if not specified, the name defaults to the jar name for the job - which is derived from the used table name. 

***You must supply the generic arguments -conf, -D, and so on after the tool name but before any tool-specific arguments (such as --connect).***

## Using Options Files to Pass Arguments



## Difference between --append and --incremental append in sqoop

### --append
Append data to an existing dataset in HDFS

--append\
--where "dpt_id >10"

is same as\
ONLY appends the data to existing data-sets, can also append duplicates -\
***NOTE: this will NOT overwrite the data but will append***

--incremental append\
--check-column dpt_id\
--last-value 10

but NOT following options it Appends the new data and Updates the existing data with new rows but - NO duplicates -\
***NOTE: this will not overwrite the data but will update OR append***

--incremental lastmodified\
--check-column lastupdated\
--last-value 20160802000000

Sqoop supports two types of incremental imports: append and lastmodified.

You can use the --incremental argument to specify the type of incremental import to perform.

append:

    You should specify append mode when importing a table where new rows are continually being added with increasing row id values.
    You specify the column containing the rowâ€™s id with --check-column.
    Sqoop imports rows where the check column has a value greater than the one specified with --last-value.\
	
NOTE: This does ***not track updated records or deleted records*** as only rows greater than "last-value" will be considered.
	

lastmodified:

    An alternate table update strategy supported by Sqoop is called lastmodified mode. You should use this when rows of the source table may be updated, and each such update will set the value of a last-modified column to the current timestamp.
    Rows where the check column holds a timestamp more recent than the timestamp specified with --last-value are imported.
    When running a subsequent import, you should specify --last-value in this way to ensure you import only the new or updated data.
    This is handled automatically by creating an incremental import as a saved job, which is the preferred mechanism for performing a recurring incremental import.
NOTE: This does ***not track deleted records*** as only rows greater than "last-value" will be considered.

https://medium.com/datadriveninvestor/incremental-data-load-using-apache-sqoop-3c259308f65c